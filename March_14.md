# Transformer: seq2seq model with self-attention

Bert: unsupervise trained transfoemer

